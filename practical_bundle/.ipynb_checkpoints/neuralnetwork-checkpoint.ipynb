{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "    # initialize the list of weights matrices, then store the\n",
    "    # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "    \n",
    "        #looping from the index of the first layer but stop before last 2 layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # initialize a weight matrix connecting the number of nodes in each \n",
    "            # layer together, add an extra node for the bias\n",
    "            w = np.random.randn(layers[i]+1, layers[i+1]+1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "            print(\"w.shape:{}\".format(w.shape))\n",
    "\n",
    "            # the last two layers are a special case where the input \n",
    "            # connections need a bias term but the output does not\n",
    "            w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "            self.W.append(w / np.sqrt(layers[-2]))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # construct and return a string that represents the network architecture\n",
    "        return \"NeuralNetwork: {}\".format(\"_\".join(str(l) for l in self.layers))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING that 'x'\n",
    "        # has already been passed through the sigmoid function\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        # add bias\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        #loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point and train\n",
    "            for (x, target) in zip(X,y):\n",
    "                self.fit_partial(x, target)\n",
    "            \n",
    "            #check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X,y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(epoch + 1, loss))\n",
    "                \n",
    "    def fit_partial(self, x, y):\n",
    "        # construct out list of output activations for each layer as data point\n",
    "        # flows through the network; the first activation is a special case -- its\n",
    "        # just the input feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        #Feedforward\n",
    "        #loop over layers in the network\n",
    "        for layer in np.arange(0,len(self.W)):\n",
    "            print(\"len of W:{}\".format(len(self.W)))\n",
    "            # feedforward the activation at the current layer by\n",
    "            # Activation * weight, this is called 'net input' \n",
    "            # A is computed from last layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            \n",
    "            out = self.sigmoid(net)\n",
    "            \n",
    "            #once we have the net output, add it to our list of activations\n",
    "            A.append(out)\n",
    "        \n",
    "        # Backpropagation\n",
    "        # the first phase of bp is compute the error between \n",
    "        # prediction and target\n",
    "        \n",
    "        error = A[-1] - y\n",
    "        \n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas ‘D‘; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        \n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "        \n",
    "        #once given the D for the final layer, we can work backforward by loop\n",
    "        for layer in np.arange(len(A) - 2, 0 , -1):\n",
    "            # the delta for the current layer is equal to the delta of the\n",
    "            # *previous layer* dotted by *weight matrix* of the current layer,\n",
    "            # followed by multiplying the delta by the derivative of the activation\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "            \n",
    "            #since we looped over all the layers in reverse order we need to reverse\n",
    "            # the deltas\n",
    "            D = D[::-1]\n",
    "\n",
    "            # WEIGHT UPDATE PHASE\n",
    "            # loop over the layers\n",
    "            for layer in np.arange(0, len(self.W)):\n",
    "                self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "    \n",
    "    def predict(self, X, addBias = True):\n",
    "        # initialize the output prediction as the input features -- this value\n",
    "        # will be (forward) propagated through the network to obtain the final\n",
    "        # prediction\n",
    "        \n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # check to see if the bias should be added\n",
    "        \n",
    "        if addBias:\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "        \n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "            # just dot from first layer to final layer\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    def calculate_loss(self, X, targets):\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        \n",
    "        return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
